{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday Meeting \n",
    "\n",
    "\n",
    "I have adapted the MLP dataprovider to the accelerometer data set to be used. I found the NN coded by Pawel extremely well documented and useful and thus I have picked it over Lasagne, tensorflow or the horrendous scikit learn (on which I wasted a whole day pointlessly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=125, odim=125*2, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125*2, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7296, 500) (7296,)\n",
      "(7296, 125)\n",
      "(1824, 500)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.988. Accuracy is 4.85%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.980. Accuracy is 4.61%\n",
      "mlp/optimisers.py:186: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  % (self.lr_scheduler.epoch + 1, cost_name, tr_nll, tr_acc * 100.))\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.984. Accuracy is 12.96%\n",
      "mlp/optimisers.py:194: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  % (self.lr_scheduler.epoch + 1, cost_name, valid_nll, valid_acc * 100.))\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.884. Accuracy is 12.72%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 1 seconds. Training speed 13587 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.991. Accuracy is 12.93%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.798. Accuracy is 14.42%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 1 seconds. Training speed 13848 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 2.876. Accuracy is 16.03%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 2.767. Accuracy is 13.54%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 1 seconds. Training speed 13848 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 2.833. Accuracy is 16.81%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 2.792. Accuracy is 18.86%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 1 seconds. Training speed 13587 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 2.821. Accuracy is 19.89%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 3.911. Accuracy is 9.05%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 2.983. Accuracy is 15.01%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 2.433. Accuracy is 21.88%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 1 seconds. Training speed 12859 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 2.233. Accuracy is 29.43%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 2.076. Accuracy is 30.65%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 1 seconds. Training speed 13093 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 2.251. Accuracy is 28.82%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 2.567. Accuracy is 21.00%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 1 seconds. Training speed 13335 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 2.213. Accuracy is 29.69%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 2.050. Accuracy is 31.03%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 1 seconds. Training speed 12002 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.971. Accuracy is 34.75%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 1.897. Accuracy is 38.43%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 1.889. Accuracy is 37.88%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 1.860. Accuracy is 40.30%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 1.789. Accuracy is 41.11%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 1.699. Accuracy is 43.70%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 1.759. Accuracy is 43.03%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 1.709. Accuracy is 43.86%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 1 seconds. Training speed 13335 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 1.800. Accuracy is 40.29%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 1.826. Accuracy is 36.95%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 1 seconds. Training speed 13335 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 2.456. Accuracy is 26.57%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 2.122. Accuracy is 38.10%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 1 seconds. Training speed 13587 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 1.951. Accuracy is 37.78%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 1.795. Accuracy is 44.52%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 1 seconds. Training speed 13587 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 1.867. Accuracy is 40.12%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 2.194. Accuracy is 30.21%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 2.184. Accuracy is 32.24%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 2.102. Accuracy is 31.47%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 1 seconds. Training speed 13587 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 2.014. Accuracy is 34.82%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 1.980. Accuracy is 33.22%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 1 seconds. Training speed 13848 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 3.134. Accuracy is 17.08%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 2.650. Accuracy is 25.33%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 1 seconds. Training speed 12002 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 2.654. Accuracy is 21.72%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 2.368. Accuracy is 23.46%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 1 seconds. Training speed 12416 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 2.429. Accuracy is 26.86%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 2.841. Accuracy is 31.03%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 1 seconds. Training speed 12205 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 2.353. Accuracy is 29.17%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 2.176. Accuracy is 29.11%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 2.315. Accuracy is 30.60%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 2.349. Accuracy is 34.16%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 1 seconds. Training speed 13335 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 2.221. Accuracy is 31.38%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 2.107. Accuracy is 32.57%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 1 seconds. Training speed 13093 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 2.228. Accuracy is 31.36%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 2.459. Accuracy is 29.28%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 1 seconds. Training speed 12416 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 2.231. Accuracy is 31.94%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 2.244. Accuracy is 26.43%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 1 seconds. Training speed 11805 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 2.174. Accuracy is 32.56%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 2.145. Accuracy is 33.22%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 1 seconds. Training speed 13848 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 2.104. Accuracy is 33.64%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 2.069. Accuracy is 33.66%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 1 seconds. Training speed 13587 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 2.134. Accuracy is 33.24%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 2.049. Accuracy is 34.38%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 1 seconds. Training speed 13848 pps. Validation speed 36500 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "/afs/inf.ed.ac.uk/user/s12/s1235260/.local/lib/python2.7/site-packages/ipykernel/__main__.py:56: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "INFO:root:MNIST test set accuracy is 34.38 %, cost (ce) is 2.049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1824,)\n",
      "(1824, 125)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(ComplexLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=2*125, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.07\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "#Every activation function from dft layer produces two values (x,y) for x+iy\n",
    "model.add_layer(Sigmoid(idim=125*2, odim=125*2, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125*2, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
