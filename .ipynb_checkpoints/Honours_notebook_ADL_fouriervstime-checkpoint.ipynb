{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday Meeting \n",
    "\n",
    "\n",
    "I have adapted the MLP dataprovider to the accelerometer data set to be used. I found the NN coded by Pawel extremely well documented and useful and thus I have picked it over Lasagne, tensorflow or the horrendous scikit learn (on which I wasted a whole day pointlessly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=-10, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax, Relu #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 78\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Linear(idim=125, odim=125*2, irange=1.6, rng=rng))\n",
    "model.add_layer(Relu(idim=125*2, odim=125*2, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125*2, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7296, 5625) (7296,)\n",
      "(7296, 125)\n",
      "(1824, 5625)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 21.188. Accuracy is 6.67%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 21.409. Accuracy is 5.70%\n",
      "/afs/inf.ed.ac.uk/user/s12/s1235260/.local/lib/python2.7/site-packages/matplotlib/backend_bases.py:2437: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 17.992. Accuracy is 23.44%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.406. Accuracy is 36.57%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 1 seconds. Training speed 13587 pps. Validation speed 182500 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.946. Accuracy is 43.11%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.678. Accuracy is 47.20%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 0 seconds. Training speed 15654 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.600. Accuracy is 51.29%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.291. Accuracy is 56.36%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.425. Accuracy is 54.68%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.358. Accuracy is 54.61%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 0 seconds. Training speed 15654 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.266. Accuracy is 58.29%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 1.164. Accuracy is 60.03%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.216. Accuracy is 61.04%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 1.297. Accuracy is 56.74%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.190. Accuracy is 60.89%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 1.219. Accuracy is 62.50%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 0 seconds. Training speed 15654 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.230. Accuracy is 62.38%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 1.057. Accuracy is 63.21%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.180. Accuracy is 64.15%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 1.067. Accuracy is 64.36%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 1 seconds. Training speed 14120 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.153. Accuracy is 64.76%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 1.029. Accuracy is 65.19%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 0 seconds. Training speed 15654 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 1.173. Accuracy is 65.60%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 1.036. Accuracy is 63.21%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 1.196. Accuracy is 66.21%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 1.812. Accuracy is 62.99%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 1.119. Accuracy is 66.42%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 1.057. Accuracy is 66.94%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 1.248. Accuracy is 66.19%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 1.181. Accuracy is 65.08%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 1.322. Accuracy is 66.56%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 1.064. Accuracy is 66.01%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 0 seconds. Training speed 15654 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 1.184. Accuracy is 68.15%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 1.407. Accuracy is 65.08%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 1.121. Accuracy is 68.44%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 1.099. Accuracy is 68.42%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 0 seconds. Training speed 15654 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 1.013. Accuracy is 69.82%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 1.396. Accuracy is 66.12%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 1 seconds. Training speed 15321 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 1.103. Accuracy is 69.51%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.911. Accuracy is 67.16%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 1.051. Accuracy is 70.61%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.912. Accuracy is 71.38%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "/afs/inf.ed.ac.uk/user/s12/s1235260/.local/lib/python2.7/site-packages/matplotlib/pyplot.py:516: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 1.011. Accuracy is 71.43%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 1.138. Accuracy is 66.89%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.980. Accuracy is 71.97%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 1.209. Accuracy is 67.87%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.976. Accuracy is 70.97%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.916. Accuracy is 67.49%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.972. Accuracy is 71.67%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.866. Accuracy is 73.08%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.962. Accuracy is 72.42%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 1.932. Accuracy is 64.14%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 1 seconds. Training speed 14402 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.993. Accuracy is 72.19%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.820. Accuracy is 71.66%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.883. Accuracy is 74.07%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 1.307. Accuracy is 67.71%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.919. Accuracy is 73.51%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.956. Accuracy is 72.09%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 1 seconds. Training speed 12002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.863. Accuracy is 74.69%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.870. Accuracy is 70.34%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.839. Accuracy is 74.74%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.912. Accuracy is 71.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.908. Accuracy is 74.24%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.909. Accuracy is 69.08%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.900. Accuracy is 74.22%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 1.391. Accuracy is 70.34%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 1 seconds. Training speed 14402 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.831. Accuracy is 74.83%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 1.051. Accuracy is 70.34%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 1 seconds. Training speed 14402 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.813. Accuracy is 75.94%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.829. Accuracy is 72.31%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 1 seconds. Training speed 13335 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.834. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.802. Accuracy is 74.34%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 1 seconds. Training speed 13587 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.838. Accuracy is 75.18%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.859. Accuracy is 73.96%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 1 seconds. Training speed 13335 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.791. Accuracy is 76.21%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.851. Accuracy is 75.00%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 1 seconds. Training speed 13587 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.815. Accuracy is 75.58%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.723. Accuracy is 75.27%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 1 seconds. Training speed 14696 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.727. Accuracy is 77.07%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.779. Accuracy is 72.26%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 1 seconds. Training speed 13587 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.727. Accuracy is 76.99%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.789. Accuracy is 74.40%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 1 seconds. Training speed 14402 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.740. Accuracy is 77.18%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.820. Accuracy is 70.01%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 1 seconds. Training speed 13848 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.717. Accuracy is 77.54%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 1.100. Accuracy is 72.53%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.763. Accuracy is 77.49%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 1.102. Accuracy is 74.67%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 1 seconds. Training speed 13848 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.726. Accuracy is 78.01%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.882. Accuracy is 72.81%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.757. Accuracy is 76.64%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.729. Accuracy is 73.90%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.724. Accuracy is 77.81%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.713. Accuracy is 75.88%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 1 seconds. Training speed 11805 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.761. Accuracy is 77.72%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.868. Accuracy is 73.03%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.741. Accuracy is 78.17%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.681. Accuracy is 76.97%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 1 seconds. Training speed 13587 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.700. Accuracy is 78.04%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.668. Accuracy is 73.08%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.678. Accuracy is 77.51%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.917. Accuracy is 72.48%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 51: Training cost (ce) is 0.686. Accuracy is 78.24%\n",
      "INFO:mlp.optimisers:Epoch 51: Validation cost (ce) is 0.919. Accuracy is 72.04%\n",
      "INFO:mlp.optimisers:Epoch 51: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 52: Training cost (ce) is 0.666. Accuracy is 78.65%\n",
      "INFO:mlp.optimisers:Epoch 52: Validation cost (ce) is 0.703. Accuracy is 74.62%\n",
      "INFO:mlp.optimisers:Epoch 52: Took 0 seconds. Training speed 15002 pps. Validation speed 182500 pps.\n",
      "INFO:mlp.optimisers:Epoch 53: Training cost (ce) is 0.685. Accuracy is 79.35%\n",
      "INFO:mlp.optimisers:Epoch 53: Validation cost (ce) is 0.667. Accuracy is 76.75%\n",
      "INFO:mlp.optimisers:Epoch 53: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 54: Training cost (ce) is 0.642. Accuracy is 79.22%\n",
      "INFO:mlp.optimisers:Epoch 54: Validation cost (ce) is 0.772. Accuracy is 76.81%\n",
      "INFO:mlp.optimisers:Epoch 54: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 55: Training cost (ce) is 0.675. Accuracy is 79.10%\n",
      "INFO:mlp.optimisers:Epoch 55: Validation cost (ce) is 0.996. Accuracy is 73.52%\n",
      "INFO:mlp.optimisers:Epoch 55: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 56: Training cost (ce) is 0.655. Accuracy is 79.32%\n",
      "INFO:mlp.optimisers:Epoch 56: Validation cost (ce) is 0.728. Accuracy is 75.99%\n",
      "INFO:mlp.optimisers:Epoch 56: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 57: Training cost (ce) is 0.649. Accuracy is 79.39%\n",
      "INFO:mlp.optimisers:Epoch 57: Validation cost (ce) is 0.744. Accuracy is 73.30%\n",
      "INFO:mlp.optimisers:Epoch 57: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 58: Training cost (ce) is 0.641. Accuracy is 79.69%\n",
      "INFO:mlp.optimisers:Epoch 58: Validation cost (ce) is 0.727. Accuracy is 74.62%\n",
      "INFO:mlp.optimisers:Epoch 58: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 59: Training cost (ce) is 0.638. Accuracy is 78.94%\n",
      "INFO:mlp.optimisers:Epoch 59: Validation cost (ce) is 0.852. Accuracy is 72.53%\n",
      "INFO:mlp.optimisers:Epoch 59: Took 1 seconds. Training speed 14696 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 60: Training cost (ce) is 0.627. Accuracy is 79.58%\n",
      "INFO:mlp.optimisers:Epoch 60: Validation cost (ce) is 0.761. Accuracy is 73.57%\n",
      "INFO:mlp.optimisers:Epoch 60: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 61: Training cost (ce) is 0.573. Accuracy is 80.47%\n",
      "INFO:mlp.optimisers:Epoch 61: Validation cost (ce) is 0.748. Accuracy is 77.69%\n",
      "INFO:mlp.optimisers:Epoch 61: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 62: Training cost (ce) is 0.631. Accuracy is 79.12%\n",
      "INFO:mlp.optimisers:Epoch 62: Validation cost (ce) is 0.708. Accuracy is 74.89%\n",
      "INFO:mlp.optimisers:Epoch 62: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 63: Training cost (ce) is 0.612. Accuracy is 79.92%\n",
      "INFO:mlp.optimisers:Epoch 63: Validation cost (ce) is 0.694. Accuracy is 77.58%\n",
      "INFO:mlp.optimisers:Epoch 63: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 64: Training cost (ce) is 0.655. Accuracy is 79.22%\n",
      "INFO:mlp.optimisers:Epoch 64: Validation cost (ce) is 0.839. Accuracy is 74.78%\n",
      "INFO:mlp.optimisers:Epoch 64: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 65: Training cost (ce) is 0.619. Accuracy is 80.03%\n",
      "INFO:mlp.optimisers:Epoch 65: Validation cost (ce) is 0.647. Accuracy is 77.19%\n",
      "INFO:mlp.optimisers:Epoch 65: Took 1 seconds. Training speed 11805 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 66: Training cost (ce) is 0.630. Accuracy is 79.38%\n",
      "INFO:mlp.optimisers:Epoch 66: Validation cost (ce) is 0.820. Accuracy is 75.82%\n",
      "INFO:mlp.optimisers:Epoch 66: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 67: Training cost (ce) is 0.612. Accuracy is 79.43%\n",
      "INFO:mlp.optimisers:Epoch 67: Validation cost (ce) is 0.713. Accuracy is 75.71%\n",
      "INFO:mlp.optimisers:Epoch 67: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 68: Training cost (ce) is 0.610. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 68: Validation cost (ce) is 0.814. Accuracy is 73.25%\n",
      "INFO:mlp.optimisers:Epoch 68: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 69: Training cost (ce) is 0.587. Accuracy is 79.56%\n",
      "INFO:mlp.optimisers:Epoch 69: Validation cost (ce) is 0.675. Accuracy is 76.37%\n",
      "INFO:mlp.optimisers:Epoch 69: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 70: Training cost (ce) is 0.622. Accuracy is 80.04%\n",
      "INFO:mlp.optimisers:Epoch 70: Validation cost (ce) is 1.016. Accuracy is 68.97%\n",
      "INFO:mlp.optimisers:Epoch 70: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 71: Training cost (ce) is 0.599. Accuracy is 80.00%\n",
      "INFO:mlp.optimisers:Epoch 71: Validation cost (ce) is 0.713. Accuracy is 75.16%\n",
      "INFO:mlp.optimisers:Epoch 71: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 72: Training cost (ce) is 0.603. Accuracy is 79.83%\n",
      "INFO:mlp.optimisers:Epoch 72: Validation cost (ce) is 0.735. Accuracy is 75.66%\n",
      "INFO:mlp.optimisers:Epoch 72: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 73: Training cost (ce) is 0.640. Accuracy is 79.44%\n",
      "INFO:mlp.optimisers:Epoch 73: Validation cost (ce) is 0.867. Accuracy is 73.96%\n",
      "INFO:mlp.optimisers:Epoch 73: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 74: Training cost (ce) is 0.622. Accuracy is 78.97%\n",
      "INFO:mlp.optimisers:Epoch 74: Validation cost (ce) is 0.657. Accuracy is 77.91%\n",
      "INFO:mlp.optimisers:Epoch 74: Took 0 seconds. Training speed 15321 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 75: Training cost (ce) is 0.582. Accuracy is 79.74%\n",
      "INFO:mlp.optimisers:Epoch 75: Validation cost (ce) is 0.641. Accuracy is 77.19%\n",
      "INFO:mlp.optimisers:Epoch 75: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:mlp.optimisers:Epoch 76: Training cost (ce) is 0.603. Accuracy is 80.12%\n",
      "INFO:mlp.optimisers:Epoch 76: Validation cost (ce) is 0.796. Accuracy is 72.81%\n",
      "INFO:mlp.optimisers:Epoch 76: Took 0 seconds. Training speed 15321 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 77: Training cost (ce) is 0.562. Accuracy is 79.97%\n",
      "INFO:mlp.optimisers:Epoch 77: Validation cost (ce) is 0.686. Accuracy is 74.23%\n",
      "INFO:mlp.optimisers:Epoch 77: Took 0 seconds. Training speed 15002 pps. Validation speed 91250 pps.\n",
      "INFO:mlp.optimisers:Epoch 78: Training cost (ce) is 0.579. Accuracy is 80.33%\n",
      "INFO:mlp.optimisers:Epoch 78: Validation cost (ce) is 0.619. Accuracy is 77.63%\n",
      "INFO:mlp.optimisers:Epoch 78: Took 1 seconds. Training speed 15002 pps. Validation speed 60833 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 77.63 %, cost (ce) is 0.619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1824,)\n",
      "(1824, 125)\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 78\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(ComplexLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Relu(idim=2*125, odim=125*2, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125*2, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.06\n",
    "max_epochs = 500\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "#Every activation function from dft layer produces two values (x,y) for x+iy\n",
    "model.add_layer(Sigmoid(idim=125*2, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.1\n",
    "max_epochs = 500\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTAugLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "#Every activation function from dft layer produces two values (x,y) for x+iy\n",
    "model.add_layer(Sigmoid(idim=125*3, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
