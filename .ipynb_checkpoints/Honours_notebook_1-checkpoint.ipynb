{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday Meeting \n",
    "\n",
    "\n",
    "I have adapted the MLP dataprovider to the accelerometer data set to be used. I found the NN coded by Pawel extremely well documented and useful and thus I have picked it over Lasagne, tensorflow or the horrendous scikit learn (on which I wasted a whole day pointlessly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=500, odim=500, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=500, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7296, 500) (7296,)\n",
      "(1824, 500)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 3.276. Accuracy is 4.81%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 3.293. Accuracy is 4.17%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 10.869. Accuracy is 33.58%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 4.751. Accuracy is 42.93%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 2527 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 5.525. Accuracy is 51.39%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 3.536. Accuracy is 57.18%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 2590 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 3.602. Accuracy is 59.72%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 3.426. Accuracy is 64.58%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 2518 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 2.858. Accuracy is 63.22%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 2.764. Accuracy is 67.00%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 2500 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 2.781. Accuracy is 65.92%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 3.050. Accuracy is 60.91%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 2590 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.727. Accuracy is 70.93%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 1.243. Accuracy is 75.05%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 3 seconds. Training speed 2581 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.765. Accuracy is 71.74%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 1.348. Accuracy is 78.02%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 2600 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.484. Accuracy is 74.67%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 1.889. Accuracy is 78.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 2509 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.243. Accuracy is 75.53%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 1.167. Accuracy is 73.14%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 2600 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.548. Accuracy is 74.35%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 2.063. Accuracy is 76.86%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 2590 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 1.428. Accuracy is 76.47%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 1.040. Accuracy is 77.91%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 2600 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 1.141. Accuracy is 78.44%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.991. Accuracy is 78.51%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 2509 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 1.197. Accuracy is 77.53%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 1.319. Accuracy is 79.00%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 2527 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 1.029. Accuracy is 79.08%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 1.103. Accuracy is 78.23%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 2384 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 1.026. Accuracy is 78.32%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 1.910. Accuracy is 71.98%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 2384 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.911. Accuracy is 80.93%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.610. Accuracy is 81.09%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 2581 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.963. Accuracy is 80.17%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.598. Accuracy is 81.20%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 2545 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 1.026. Accuracy is 80.67%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 1.385. Accuracy is 78.78%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 2449 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.894. Accuracy is 80.86%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.578. Accuracy is 83.33%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 2257 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.899. Accuracy is 80.65%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 1.370. Accuracy is 79.17%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 2600 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.792. Accuracy is 82.29%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.852. Accuracy is 79.88%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 2600 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.639. Accuracy is 83.88%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 1.125. Accuracy is 78.67%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 2590 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.766. Accuracy is 82.72%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.865. Accuracy is 78.29%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 2272 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.668. Accuracy is 83.83%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 1.498. Accuracy is 79.88%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 2600 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.725. Accuracy is 83.50%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.716. Accuracy is 81.52%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 2609 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.660. Accuracy is 84.22%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.925. Accuracy is 78.12%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 2353 pps. Validation speed 10139 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.628. Accuracy is 84.99%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.901. Accuracy is 77.03%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 2441 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.601. Accuracy is 84.53%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.697. Accuracy is 82.02%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 2545 pps. Validation speed 12167 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.656. Accuracy is 84.42%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.544. Accuracy is 84.21%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 2563 pps. Validation speed 11406 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.522. Accuracy is 86.60%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.665. Accuracy is 80.81%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 2572 pps. Validation speed 11406 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 80.81 %, cost (ce) is 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1824,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(ComplexLinear(idim=500, odim=500, irange=1.6, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=500, irange=1.6, rng=rng))\n",
    "#model.add_layer(Sigmoid(idim=500, odim=500, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=500, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
