{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3rd Group Presentation\n",
    "\n",
    "## Reminder of The Project\n",
    "\n",
    "The Idea behind this project is to study techniques from fourier analysis / digital signal processing in conjunction with machine learning methods in order to provide a good classification framework for time series based signals comming from accelerometers with suspected periodic behaviour.\n",
    "\n",
    "The original context of classification was to detect changes in between two classes or states of the underlying subject from which the time series metrics originated.\n",
    "\n",
    "## Data sets involved\n",
    "\n",
    "### http://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities\n",
    "\n",
    "Constitutes of 19 different sport/ daily activities. In which 25hz samples are obtainned from accelerometers, gyroscopes and magnetometers attached to LA, RA, T, LL, RL. 5 seconds per task 8 different people involved in this study a total of \n",
    "9120 data points. I have worked on this dataset so far planning on working with the others as soon as I finsih some aspects of my framework which need building/optimizing.\n",
    "\n",
    "###  http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\n",
    "\n",
    "This dataset is measured from the accelerometers and gyroscopes in smartphones. Class labels are 3 postures and 3 activities and the number of instances 10929. The advantage of this data set is that it is more timeseries in the sense that the samples contain the transitions from one activity to another which matches more the original data set and proposed problem.\n",
    "\n",
    "### IceRobotics data set\n",
    "\n",
    "There was a problem obtaining the dataset with 800 instances due to ownership issues involved in the work carried out on the dataset. This was a major set back. What the company proposed was to give a smaller dataset of ~ 100 samples but due to the small statistical value in this size I decided to work on other similar datasets and then later on if there is time test things out with this one.\n",
    "\n",
    "\n",
    "\n",
    "## Choice of Technologoes and Frameworks\n",
    "\n",
    "I played around with scikit learn and lasagne and quickly discovered that they have some limitations and can be quite sensitive when doing changes such as pre initliazing the weight matrix. So I decided to work the the framework we built in Machine learning practical since I worked on it for a very long period of time during the holidays and know it rather well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=-10, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Experiment\n",
    "\n",
    "The following experiment uses the right arm y-reading of the accelerometer as its input space on a double hidden layer one linear and one relu activation function based neural network. The choice of hidden units and topology of this baseline experiment was chosen to asses the performance of a fourier transform on the input space vs the original input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax, Relu #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 200\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "# NETWORK TOPOLOGY:\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Relu(idim=125, odim=2*125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=2*125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats_b, valid_stats_b = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "a = plt.plot(np.array(tr_stats)[:,1])\n",
    "b = plt.plot(np.array(valid_stats)[:,1])\n",
    "plt.legend([a[0], b[0]], [\"train\", \"valid\"], loc=3)\n",
    "plt.xlabel(\"Epoch\");\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "col_func = (lambda x: \"#%06x\" % random.randint(0, 0xFFFFFF))\n",
    "\n",
    "def plot_stats(stats, mode='train_acc', shds=None, corr=False, max_epochs=30, figsize=(15,25)):\n",
    "    title = str(mode)\n",
    "    mode_dict = {'train_acc': (0,1),\n",
    "                 'val_acc': (1,1),\n",
    "                 'test_cost': (2,0),\n",
    "                 'test_acc': (2,1)}\n",
    "    if not corr:\n",
    "        mode = mode_dict[mode]\n",
    "        plt.figure(figsize=(10,10))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(len(stats),figsize=figsize, sharex=True,sharey=True)\n",
    "        ax = [ax]\n",
    "        fig.suptitle(title)\n",
    "    a = list()\n",
    "    colors = [\"#%06x\" % random.randint(0, 0xFFFFFF)for i in range(len(stats))]\n",
    "    \n",
    "    for j, plo in enumerate(stats):\n",
    "        \n",
    "        if not corr:\n",
    "            a.append(plt.plot(range(len(numpy.array(plo[mode[0]])[:,mode[1]][:max_epochs])),\n",
    "                      numpy.array(plo[mode[0]])[:,mode[1]][:max_epochs] , '-o', ms=2))\n",
    "        else:\n",
    "            mode = mode_dict[corr[0]]\n",
    "            mode2 = mode_dict[corr[1]]\n",
    "           \n",
    "            train_cost = numpy.array(plo[mode[0]])[1:,mode[1]][1:max_epochs]\n",
    "            test_cost = numpy.array(plo[mode2[0]])[1:,mode2[1]][1:max_epochs]\n",
    "    \n",
    "            # euclidean distance of each cost pair from origin: (a metric of their combined error)\n",
    "            euclid = np.sqrt(test_cost*test_cost + train_cost*train_cost )\n",
    "            mx_ts = numpy.max(numpy.log(test_cost))\n",
    "            mn_ts = numpy.min(numpy.log(test_cost))\n",
    "            # Discretization gets carried out in the next two lines out of our\n",
    "            # euclid error metric for the bubble plot\n",
    "            hz, bi = numpy.histogram(euclid, bins=65)\n",
    "            sz = list(chain(*[[20*c]*hz[c-1]  for c in range(1,len(hz)+1)]))\n",
    "            \n",
    "            norm = numpy.sum(euclid)\n",
    "\n",
    "            label = ax[j].scatter((train_cost),\n",
    "                                  (test_cost),\n",
    "                                  s=sz, alpha=0.5, color= colors[j])\n",
    "            ax[j].set_xlabel(\"$log($\"+title.split(\"vs\")[-1]+\"$)$\")\n",
    "            ax[j].set_ylabel(\"$log($\"+title.split(\"vs\")[0]+\"$)$\")\n",
    "            \n",
    "\n",
    "    \n",
    "    if not corr:\n",
    "        plt.legend(map(lambda x: x[0],a), shds,loc=5)\n",
    "        plt.xlabel(\"epoch number\")\n",
    "        plt.ylabel(title.split(\"_\")[-1])\n",
    "        \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats1  = [(tr_stats, valid_stats, (tst_cost, tst_accuracy))]\n",
    "me=78\n",
    "plot_stats(stats1,'val cost vs train cost',\n",
    "           shds=[\"basline\"], corr=('val_acc', 'train_acc') ,max_epochs=me, figsize= (15,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Layer\n",
    "\n",
    "Instead of transforming the data in to fourier via fft or some other dft scipy function I decided to do this by creating a neural network layer whose weight matrix is initliazed to the DFT matrix  \n",
    "\n",
    "![](https://raw.githubusercontent.com/franciscovargas/MLPHonoursExtension/master/1.png)\n",
    "\n",
    "Where :\n",
    "\n",
    "\\begin{equation}\n",
    "\\omega_{j+1k+1}= e^{\\frac{-2\\pi i*j*k}{2}} \\quad j,k \\in \\{0,1\\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_{1} = \\beta_{2} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_{0} = 1\n",
    "\\end{equation}\n",
    "In our case example above where the input and the fourier space live in $\\mathbb{R}^2$\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Due to the parallel nature of the neural network layer this structure allows for an efficient GPU computation of a fourier transform;\n",
    "* One can extend the backpropagation algorithm to update the weights in the fourier layer which is a way of relaxing this transformation to the data. \n",
    "\n",
    "### Modification to the Network Architecture Due to Complex numbers\n",
    "\n",
    "Since $f_{i} \\in \\mathbb{C}$ we need to represent them in a real form such that we can effectively carry out backwards and forward propagation. Taking the square magnitude of it makes the gradients rather ugly thus the easiest way is to map the complex number to a vector in the following form:\n",
    "\n",
    "\\begin{equation}\n",
    " x + iy \\rightarrow (x,y)\n",
    "\\end{equation}\n",
    "\n",
    "If we wish to represent this in a network architecture we need to double the number of neurons : \n",
    "\n",
    "![](https://raw.githubusercontent.com/franciscovargas/MLPHonoursExtension/master/2.png)\n",
    "\n",
    "We do the mapping shown above for each element in the weight matrix and this is why we get double the number of neurons in the hidden fourier layer.\n",
    "\n",
    "\n",
    "### Normalization Attempts\n",
    "\n",
    "Neural networks are very sensitive to initializing the weights at particular values. Thus normalizing the DFT weight vectors to give equal mean squared magnitude to the DFT components was attempted in the following manner (normalizing factor for the $j^{th}$ fourier weight vector):\n",
    "\n",
    "\\begin{equation}\n",
    "E[(XW_{j})^{2}] = \\frac{1}{d \\cdot n} \\sum_{k=1}^{n}\\sum_{i=1}^{d}(w_{jd} \\cdot X_{kd})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Where $n$ is the number of instances in the training set and $d$ is the dimensionality.\n",
    "Dividing each fourier weight vector by the corresponding normalization factor ensures that that the means square sum of the weight matrix apllied to the inputs is equal to 1. \n",
    "\n",
    "This however yields lower accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiments with relaxed DFT layer\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 500\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "# Network topolpgy\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(ComplexRelu(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125*2, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats_fr, valid_stats_fr = optimiser.train(model, train_dp, valid_dp, fft=False)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost_fr, tst_accuracy_fr = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy_fr*100., cost.get_name(), tst_cost_fr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = plt.plot(np.array(tr_stats_fr)[:,1])\n",
    "b = plt.plot(np.array(valid_stats_fr)[:,1])\n",
    "plt.legend([a[0], b[0]], [\"train\", \"valid\"], loc=3)\n",
    "plt.xlabel(\"Epoch\");\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statsfr  = [(tr_stats_fr, valid_stats_fr, (tst_cost_fr, tst_accuracy_fr))]\n",
    "me=78\n",
    "plot_stats(statsfr,'val cost vs train cost',\n",
    "           shds=[\"basline\"], corr=('val_acc', 'train_acc') ,max_epochs=me, figsize= (15,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7296, 5625) (7296,)\n",
      "(7296, 125)\n",
      "(1824, 5625)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 9.698. Accuracy is 4.85%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 9.938. Accuracy is 4.71%\n",
      "mlp/costs.py:55: RuntimeWarning: divide by zero encountered in log\n",
      "  nll = t * numpy.log(y)\n",
      "mlp/costs.py:55: RuntimeWarning: invalid value encountered in multiply\n",
      "  nll = t * numpy.log(y)\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is nan. Accuracy is 35.06%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.217. Accuracy is 46.60%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.878. Accuracy is 48.92%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.551. Accuracy is 50.99%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.528. Accuracy is 52.07%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.327. Accuracy is 55.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.406. Accuracy is 53.43%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.375. Accuracy is 52.74%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.280. Accuracy is 56.07%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 1.215. Accuracy is 57.07%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.160. Accuracy is 57.96%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 1.133. Accuracy is 59.10%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.138. Accuracy is 59.04%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 1.091. Accuracy is 60.20%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 0 seconds. Training speed 24831 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.080. Accuracy is 61.21%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 1.272. Accuracy is 58.66%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 0 seconds. Training speed 23229 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.039. Accuracy is 62.18%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 1.172. Accuracy is 59.59%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 0 seconds. Training speed 23229 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.018. Accuracy is 62.86%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 1.090. Accuracy is 60.36%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 1.000. Accuracy is 63.08%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 1.189. Accuracy is 60.36%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.944. Accuracy is 64.35%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 1.133. Accuracy is 60.80%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.928. Accuracy is 65.32%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 1.027. Accuracy is 62.28%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.909. Accuracy is 65.29%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 1.000. Accuracy is 63.71%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.878. Accuracy is 66.93%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 1.022. Accuracy is 62.72%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.865. Accuracy is 67.21%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.995. Accuracy is 62.99%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.849. Accuracy is 66.93%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.963. Accuracy is 64.20%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.852. Accuracy is 67.69%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.983. Accuracy is 62.61%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 0 seconds. Training speed 21821 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.824. Accuracy is 67.82%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.966. Accuracy is 64.09%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 0 seconds. Training speed 24831 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.845. Accuracy is 68.15%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.971. Accuracy is 64.31%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 0 seconds. Training speed 22503 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.811. Accuracy is 68.88%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.995. Accuracy is 63.43%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.790. Accuracy is 69.78%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.942. Accuracy is 64.75%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 0 seconds. Training speed 20574 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.783. Accuracy is 69.31%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.931. Accuracy is 65.62%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.766. Accuracy is 70.08%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 1.041. Accuracy is 63.71%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.766. Accuracy is 69.83%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.939. Accuracy is 64.25%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 0 seconds. Training speed 26670 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.761. Accuracy is 69.94%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.907. Accuracy is 65.84%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 0 seconds. Training speed 22503 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.751. Accuracy is 70.53%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.899. Accuracy is 66.34%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 0 seconds. Training speed 25718 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.733. Accuracy is 70.76%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.928. Accuracy is 66.01%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.746. Accuracy is 70.79%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.951. Accuracy is 65.08%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.734. Accuracy is 71.44%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.911. Accuracy is 66.94%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 0 seconds. Training speed 25718 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.723. Accuracy is 71.68%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.933. Accuracy is 66.89%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 0 seconds. Training speed 22503 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.735. Accuracy is 71.62%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.899. Accuracy is 66.83%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 0 seconds. Training speed 24831 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.717. Accuracy is 71.78%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.964. Accuracy is 66.23%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.698. Accuracy is 72.33%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.892. Accuracy is 67.00%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.705. Accuracy is 71.65%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.898. Accuracy is 67.05%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.693. Accuracy is 72.68%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.975. Accuracy is 65.02%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.691. Accuracy is 72.14%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.889. Accuracy is 67.21%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.667. Accuracy is 73.22%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.898. Accuracy is 66.17%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.678. Accuracy is 73.19%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 1.165. Accuracy is 66.17%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.692. Accuracy is 72.68%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.967. Accuracy is 66.23%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.676. Accuracy is 73.18%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.917. Accuracy is 66.56%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.672. Accuracy is 73.62%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.888. Accuracy is 67.05%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.661. Accuracy is 74.03%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.863. Accuracy is 69.46%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.666. Accuracy is 73.64%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.888. Accuracy is 66.83%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.650. Accuracy is 73.99%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.885. Accuracy is 68.64%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.642. Accuracy is 74.19%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.866. Accuracy is 68.04%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.633. Accuracy is 74.53%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.872. Accuracy is 68.31%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.637. Accuracy is 74.11%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.878. Accuracy is 67.65%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.616. Accuracy is 75.15%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.999. Accuracy is 65.68%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.634. Accuracy is 74.31%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.854. Accuracy is 67.27%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 51: Training cost (ce) is 0.636. Accuracy is 74.36%\n",
      "INFO:mlp.optimisers:Epoch 51: Validation cost (ce) is 0.868. Accuracy is 67.71%\n",
      "INFO:mlp.optimisers:Epoch 51: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 52: Training cost (ce) is 0.615. Accuracy is 75.47%\n",
      "INFO:mlp.optimisers:Epoch 52: Validation cost (ce) is 0.865. Accuracy is 67.87%\n",
      "INFO:mlp.optimisers:Epoch 52: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 53: Training cost (ce) is 0.610. Accuracy is 74.86%\n",
      "INFO:mlp.optimisers:Epoch 53: Validation cost (ce) is 0.869. Accuracy is 67.76%\n",
      "INFO:mlp.optimisers:Epoch 53: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 54: Training cost (ce) is 0.609. Accuracy is 74.96%\n",
      "INFO:mlp.optimisers:Epoch 54: Validation cost (ce) is 0.932. Accuracy is 66.67%\n",
      "INFO:mlp.optimisers:Epoch 54: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 55: Training cost (ce) is 0.607. Accuracy is 75.28%\n",
      "INFO:mlp.optimisers:Epoch 55: Validation cost (ce) is 0.915. Accuracy is 67.38%\n",
      "INFO:mlp.optimisers:Epoch 55: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 56: Training cost (ce) is 0.625. Accuracy is 74.89%\n",
      "INFO:mlp.optimisers:Epoch 56: Validation cost (ce) is 0.862. Accuracy is 68.86%\n",
      "INFO:mlp.optimisers:Epoch 56: Took 0 seconds. Training speed 25718 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 57: Training cost (ce) is 0.600. Accuracy is 75.72%\n",
      "INFO:mlp.optimisers:Epoch 57: Validation cost (ce) is 0.878. Accuracy is 68.53%\n",
      "INFO:mlp.optimisers:Epoch 57: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 58: Training cost (ce) is 0.597. Accuracy is 75.68%\n",
      "INFO:mlp.optimisers:Epoch 58: Validation cost (ce) is 0.868. Accuracy is 67.87%\n",
      "INFO:mlp.optimisers:Epoch 58: Took 0 seconds. Training speed 23229 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 59: Training cost (ce) is 0.595. Accuracy is 76.00%\n",
      "INFO:mlp.optimisers:Epoch 59: Validation cost (ce) is 0.848. Accuracy is 69.35%\n",
      "INFO:mlp.optimisers:Epoch 59: Took 0 seconds. Training speed 22503 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 60: Training cost (ce) is 0.586. Accuracy is 76.03%\n",
      "INFO:mlp.optimisers:Epoch 60: Validation cost (ce) is 0.863. Accuracy is 68.42%\n",
      "INFO:mlp.optimisers:Epoch 60: Took 0 seconds. Training speed 24831 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 61: Training cost (ce) is 0.591. Accuracy is 75.99%\n",
      "INFO:mlp.optimisers:Epoch 61: Validation cost (ce) is 0.874. Accuracy is 68.20%\n",
      "INFO:mlp.optimisers:Epoch 61: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 62: Training cost (ce) is 0.582. Accuracy is 76.62%\n",
      "INFO:mlp.optimisers:Epoch 62: Validation cost (ce) is 0.859. Accuracy is 68.75%\n",
      "INFO:mlp.optimisers:Epoch 62: Took 0 seconds. Training speed 22503 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 63: Training cost (ce) is 0.588. Accuracy is 76.03%\n",
      "INFO:mlp.optimisers:Epoch 63: Validation cost (ce) is 0.868. Accuracy is 68.75%\n",
      "INFO:mlp.optimisers:Epoch 63: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 64: Training cost (ce) is 0.604. Accuracy is 75.94%\n",
      "INFO:mlp.optimisers:Epoch 64: Validation cost (ce) is 0.869. Accuracy is 69.24%\n",
      "INFO:mlp.optimisers:Epoch 64: Took 0 seconds. Training speed 21821 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 65: Training cost (ce) is 0.588. Accuracy is 76.21%\n",
      "INFO:mlp.optimisers:Epoch 65: Validation cost (ce) is 0.954. Accuracy is 66.28%\n",
      "INFO:mlp.optimisers:Epoch 65: Took 0 seconds. Training speed 23229 pps. Validation speed 30417 pps.\n",
      "INFO:mlp.optimisers:Epoch 66: Training cost (ce) is 0.576. Accuracy is 76.88%\n",
      "INFO:mlp.optimisers:Epoch 66: Validation cost (ce) is 0.893. Accuracy is 67.32%\n",
      "INFO:mlp.optimisers:Epoch 66: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 67: Training cost (ce) is 0.572. Accuracy is 76.86%\n",
      "INFO:mlp.optimisers:Epoch 67: Validation cost (ce) is 0.848. Accuracy is 68.53%\n",
      "INFO:mlp.optimisers:Epoch 67: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 68: Training cost (ce) is 0.568. Accuracy is 76.71%\n",
      "INFO:mlp.optimisers:Epoch 68: Validation cost (ce) is 0.825. Accuracy is 70.45%\n",
      "INFO:mlp.optimisers:Epoch 68: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 69: Training cost (ce) is 0.567. Accuracy is 77.35%\n",
      "INFO:mlp.optimisers:Epoch 69: Validation cost (ce) is 0.942. Accuracy is 68.26%\n",
      "INFO:mlp.optimisers:Epoch 69: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 70: Training cost (ce) is 0.570. Accuracy is 77.21%\n",
      "INFO:mlp.optimisers:Epoch 70: Validation cost (ce) is 0.842. Accuracy is 69.68%\n",
      "INFO:mlp.optimisers:Epoch 70: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 71: Training cost (ce) is 0.571. Accuracy is 77.29%\n",
      "INFO:mlp.optimisers:Epoch 71: Validation cost (ce) is 0.898. Accuracy is 68.86%\n",
      "INFO:mlp.optimisers:Epoch 71: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 72: Training cost (ce) is 0.572. Accuracy is 76.88%\n",
      "INFO:mlp.optimisers:Epoch 72: Validation cost (ce) is 0.843. Accuracy is 69.24%\n",
      "INFO:mlp.optimisers:Epoch 72: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 73: Training cost (ce) is 0.567. Accuracy is 77.14%\n",
      "INFO:mlp.optimisers:Epoch 73: Validation cost (ce) is 0.891. Accuracy is 67.65%\n",
      "INFO:mlp.optimisers:Epoch 73: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 74: Training cost (ce) is 0.550. Accuracy is 77.83%\n",
      "INFO:mlp.optimisers:Epoch 74: Validation cost (ce) is 0.855. Accuracy is 69.63%\n",
      "INFO:mlp.optimisers:Epoch 74: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 75: Training cost (ce) is 0.553. Accuracy is 78.19%\n",
      "INFO:mlp.optimisers:Epoch 75: Validation cost (ce) is 0.933. Accuracy is 68.04%\n",
      "INFO:mlp.optimisers:Epoch 75: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 76: Training cost (ce) is 0.562. Accuracy is 77.29%\n",
      "INFO:mlp.optimisers:Epoch 76: Validation cost (ce) is 0.842. Accuracy is 69.13%\n",
      "INFO:mlp.optimisers:Epoch 76: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 77: Training cost (ce) is 0.550. Accuracy is 77.56%\n",
      "INFO:mlp.optimisers:Epoch 77: Validation cost (ce) is 0.838. Accuracy is 70.01%\n",
      "INFO:mlp.optimisers:Epoch 77: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 78: Training cost (ce) is 0.542. Accuracy is 77.74%\n",
      "INFO:mlp.optimisers:Epoch 78: Validation cost (ce) is 0.847. Accuracy is 69.46%\n",
      "INFO:mlp.optimisers:Epoch 78: Took 0 seconds. Training speed 24831 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 79: Training cost (ce) is 0.542. Accuracy is 78.03%\n",
      "INFO:mlp.optimisers:Epoch 79: Validation cost (ce) is 0.833. Accuracy is 69.57%\n",
      "INFO:mlp.optimisers:Epoch 79: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 80: Training cost (ce) is 0.537. Accuracy is 77.93%\n",
      "INFO:mlp.optimisers:Epoch 80: Validation cost (ce) is 0.836. Accuracy is 70.18%\n",
      "INFO:mlp.optimisers:Epoch 80: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 81: Training cost (ce) is 0.543. Accuracy is 77.83%\n",
      "INFO:mlp.optimisers:Epoch 81: Validation cost (ce) is 0.951. Accuracy is 67.76%\n",
      "INFO:mlp.optimisers:Epoch 81: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 82: Training cost (ce) is 0.539. Accuracy is 77.92%\n",
      "INFO:mlp.optimisers:Epoch 82: Validation cost (ce) is 0.854. Accuracy is 68.37%\n",
      "INFO:mlp.optimisers:Epoch 82: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 83: Training cost (ce) is 0.542. Accuracy is 78.07%\n",
      "INFO:mlp.optimisers:Epoch 83: Validation cost (ce) is 0.846. Accuracy is 70.56%\n",
      "INFO:mlp.optimisers:Epoch 83: Took 0 seconds. Training speed 26670 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 84: Training cost (ce) is 0.545. Accuracy is 78.00%\n",
      "INFO:mlp.optimisers:Epoch 84: Validation cost (ce) is 0.899. Accuracy is 68.26%\n",
      "INFO:mlp.optimisers:Epoch 84: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 85: Training cost (ce) is 0.531. Accuracy is 78.21%\n",
      "INFO:mlp.optimisers:Epoch 85: Validation cost (ce) is 0.860. Accuracy is 69.19%\n",
      "INFO:mlp.optimisers:Epoch 85: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 86: Training cost (ce) is 0.536. Accuracy is 78.07%\n",
      "INFO:mlp.optimisers:Epoch 86: Validation cost (ce) is 0.850. Accuracy is 70.07%\n",
      "INFO:mlp.optimisers:Epoch 86: Took 0 seconds. Training speed 25718 pps. Validation speed 45625 pps.\n",
      "INFO:mlp.optimisers:Epoch 87: Training cost (ce) is 0.521. Accuracy is 78.62%\n",
      "INFO:mlp.optimisers:Epoch 87: Validation cost (ce) is 0.841. Accuracy is 70.29%\n",
      "INFO:mlp.optimisers:Epoch 87: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 88: Training cost (ce) is 0.529. Accuracy is 78.68%\n",
      "INFO:mlp.optimisers:Epoch 88: Validation cost (ce) is 0.857. Accuracy is 69.52%\n",
      "INFO:mlp.optimisers:Epoch 88: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 89: Training cost (ce) is 0.525. Accuracy is 78.33%\n",
      "INFO:mlp.optimisers:Epoch 89: Validation cost (ce) is 0.836. Accuracy is 69.74%\n",
      "INFO:mlp.optimisers:Epoch 89: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 90: Training cost (ce) is 0.524. Accuracy is 78.82%\n",
      "INFO:mlp.optimisers:Epoch 90: Validation cost (ce) is 0.854. Accuracy is 68.64%\n",
      "INFO:mlp.optimisers:Epoch 90: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 91: Training cost (ce) is 0.524. Accuracy is 78.74%\n",
      "INFO:mlp.optimisers:Epoch 91: Validation cost (ce) is 0.924. Accuracy is 67.49%\n",
      "INFO:mlp.optimisers:Epoch 91: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 92: Training cost (ce) is 0.528. Accuracy is 77.93%\n",
      "INFO:mlp.optimisers:Epoch 92: Validation cost (ce) is 0.854. Accuracy is 69.02%\n",
      "INFO:mlp.optimisers:Epoch 92: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 93: Training cost (ce) is 0.525. Accuracy is 78.69%\n",
      "INFO:mlp.optimisers:Epoch 93: Validation cost (ce) is 0.826. Accuracy is 70.01%\n",
      "INFO:mlp.optimisers:Epoch 93: Took 0 seconds. Training speed 26670 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 94: Training cost (ce) is 0.511. Accuracy is 79.25%\n",
      "INFO:mlp.optimisers:Epoch 94: Validation cost (ce) is 0.866. Accuracy is 69.08%\n",
      "INFO:mlp.optimisers:Epoch 94: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 95: Training cost (ce) is 0.516. Accuracy is 79.00%\n",
      "INFO:mlp.optimisers:Epoch 95: Validation cost (ce) is 0.879. Accuracy is 69.35%\n",
      "INFO:mlp.optimisers:Epoch 95: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n",
      "INFO:mlp.optimisers:Epoch 96: Training cost (ce) is 0.518. Accuracy is 78.90%\n",
      "INFO:mlp.optimisers:Epoch 96: Validation cost (ce) is 0.829. Accuracy is 69.79%\n",
      "INFO:mlp.optimisers:Epoch 96: Took 0 seconds. Training speed 25718 pps. Validation speed 36500 pps.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate =0.1\n",
    "max_epochs = 200\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTPLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "#Every activation function from dft layer produces two values (x,y) for x+iy\n",
    "model.add_layer(Relu(idim=108, odim=2*125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=2*125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats_f, valid_stats_f = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_costf, tst_accuracyf = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracyf*100., cost.get_name(), tst_costf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 78\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "#Every activation function from dft layer produces two values (x,y) for x+iy\n",
    "model.add_layer(Relu(idim=125*2, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats_f, valid_stats_f = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_costf, tst_accuracyf = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracyf*100., cost.get_name(), tst_costf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = ACLDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = ACLDataProvider(dset='valid', batch_size=1824, max_num_batches=1, randomize=False)\n",
    "\n",
    "\n",
    "#Baseline experiment\n",
    "from copy import deepcopy\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.layers import * \n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 100\n",
    "learning_rate = 0.01\n",
    "max_epochs = 78\n",
    "cost = CECost()\n",
    "    \n",
    "stats = list()\n",
    "\n",
    "test_dp = deepcopy(valid_dp)\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(DFTAugLinear(idim=125, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Relu(idim=125*3, odim=125, irange=1.6, rng=rng))\n",
    "model.add_layer(Softmax(idim=125, odim=19, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "\n",
    "tst_cost, tst_accuracy = optimiser.validate(model,test_dp )\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assesing Performance of Devices (Shapley Value)\n",
    "\n",
    "Since the data set has a 5 different positioning of 3 different devices all with 3 different readings it provides a high combination all together there is space to study how devices cooperate with each other.\n",
    "\n",
    "\n",
    "The Shapely value from game theory is a useful metric to study how players cooperate in a particular game. Seeing the devices as players this value provides a way of distributing the total gain in the classification process (game) to every device involved.\n",
    "\n",
    "So far the implementation I have for this is slow and needs some work. I managed to test it on the right hand device where each player was an axes of the device. This small experiment yielded that the y-axis was the one with the highest gain. Which is something one would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "* Making well supported conclusions on the fourier transform based attempts. Need more supporting plots and test on the other data set;\n",
    "* Use standard pretraining techniques and other methods to otpimize both baselines and the fourier network;\n",
    "* Apply certain timeseries models (ARMA, ARIMA, ...) for feature extraction purposes and contrast against a convet;\n",
    "* STFT Layer in a convnet;\n",
    "* Extend Shapley value to look at devices as players in the classification process;\n",
    "* Look in to seasonal trends in timeseries and at the SANN network architecture;\n",
    "\n",
    "\n",
    "## STFT arch\n",
    "\n",
    "![](https://raw.githubusercontent.com/franciscovargas/MLPHonoursExtension/master/3.png)\n",
    "\n",
    "### Motivation\n",
    "\n",
    "For dynamic processes that change in time it sometimes makes sense to window the fourier transform of subsegments since the periodicity may also evolve in time and thus a spectogram becomes a good candiate for a representation in the frequency domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
